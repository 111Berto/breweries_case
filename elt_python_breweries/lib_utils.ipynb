{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec5dee7-9338-4275-87d7-cb2a3da2e809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Biblioteca(s)\n",
    "import json\n",
    "from pyspark.sql import DataFrame\n",
    "from requests.models import Response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1bfd6b3-98c0-4061-94f6-a734af9866bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fn_request_response_to_dataframe(response: Response) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converte a resposta JSON de uma solicitação HTTP em um DataFrame do PySpark.\n",
    "    \n",
    "    Args:\n",
    "    response (Response): O objeto de resposta obtido por uma solicitação HTTP usando a biblioteca requests.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Um DataFrame do PySpark contendo os dados extraídos da resposta JSON.\n",
    "    \n",
    "    Exemplo de uso:\n",
    "    >>> import requests\n",
    "    >>> url = \"https://api.exemplo.com/dados\"\n",
    "    >>> response = requests.get(url)\n",
    "    >>> df = fn_request_response_to_dataframe(response)\n",
    "    >>> df.show()\n",
    "    \n",
    "    Note que a URL usada no exemplo acima deve ser substituída por uma URL real de onde você está fazendo a solicitação HTTP.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converte a resposta JSON em um dicionário Python\n",
    "    payload = json.loads(json.dumps(response.text))\n",
    "    \n",
    "    # Cria um RDD a partir do payload usando o contexto do Spark\n",
    "    payload_rdd = spark.sparkContext.parallelize([payload])\n",
    "    \n",
    "    # Lê o RDD como um DataFrame\n",
    "    df = spark.read.json(payload_rdd)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479a29d7-01bf-4c61-bd22-d350c8138bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_database_if_not_exists(db_name: str, table_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Função para verificar e criar o database, se não existir.\n",
    "    \n",
    "    Args:\n",
    "    db_name (str): O nome do database.\n",
    "    table_path (str): O caminho onde a tabela será criada.\n",
    "    \"\"\"\n",
    "    # Criação do database se não existir\n",
    "    print(f\"Verificando se o database {db_name} existe. O mesmo será criado em {table_path} caso não exista\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{table_path}';\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lib_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
